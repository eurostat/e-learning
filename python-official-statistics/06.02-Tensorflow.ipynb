{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "<span style='background: rgb(128, 128, 128, .15); width: 100%; display: block; padding: 10px 0 10px 10px'>< [Scikit-learn](06.01-Scikit.ipynb) | [Contents](00.00-Index.ipynb) | [Quiz](06.03-Quiz.ipynb) ></span>\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/eurostat/e-learning/blob/main/python-official-statistics/06.02-Tensorflow.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "# Tensorflow + Keras and PyTorch\n",
    "## Content  \n",
    "- [What is Tensorflow?](#tensorflow)\n",
    "- [And Keras?](#keras)\n",
    "- [TensorFlow - Not Just DL](#classic)\n",
    "- [Simple linear regression](#regression)\n",
    "- [MLP Classifier: Handwritten Digits](#tf-mlp)\n",
    "- [Convolutional Neural Network](#tf-cnn)\n",
    "- [PyTorch](#pt)\n",
    "- [Transfer Learning](#transfer)\n",
    "\n",
    "``Sklearn`` doesn't have much support for Deep Neural Networks. So when you want/need to unleash the power of deep learning you need something more.  \n",
    "Luckily there are two powerful packages to work with: ``Tensorflow`` and ``PyTorch``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tensorflow'></a>\n",
    "\n",
    "## What is Tensorflow?\n",
    "TensorFlow is an open-source library developed by ``Google`` primarily for ``deep learning applications``. It also supports traditional machine learning. TensorFlow was originally developed for large numerical computations without keeping deep learning in mind.  \n",
    "TensorFlow is more of a low-level library. Basically, we can think of TensorFlow as the Lego bricks (similar to NumPy and SciPy) that we can use to implement machine learning algorithms whereas Scikit-Learn comes with off-the-shelf algorithms and has an easy to use interface.  \n",
    "Starting with version 2.0, more efficiency and convenience was brought to the game.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='keras'></a>\n",
    "\n",
    "## What is Keras?\n",
    "Keras is built on top of TensorFlow, which makes it a wrapper for deep learning purposes. It is incredibly user-friendly and easy to pick up. A solid asset is its neural network block modularity and the fact that it is written in Python, which makes it easy to debug.  \n",
    "\n",
    "`Note`: In this lecture I will refer as Tensorflow when talking about Keras + Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='classic'></a>\n",
    "\n",
    "## TensorFlow - Not Just for Deep Learning\n",
    "Yes, TensorFlow is not just for deep learning. It provides a great variety of building blocks for general numerical computation and machine learning. Next we will introduce the wide range of general machine learning algorithms and their building blocks provided by TensorFlow.  \n",
    "\n",
    "### High-level Estimators\n",
    "TensorFlow provides various number of machine learning algorithms inside itâ€™s estimators module. Besides easy-to-use deep learning APIs such as Deep Neural Networks, Recurrent Neural Networks, etc, there are also a collection of popular machine learning algorithms. Currently, the following algorithms are included:\n",
    "- K-means clustering\n",
    "- Random Forests\n",
    "- Support Vector Machines\n",
    "- Gaussian Mixture Model clustering\n",
    "- Linear/logistic regression\n",
    "\n",
    "### Statistical Distributions\n",
    "A wide variety of statistical distributions functions are also provided by TensorFlow, including but not limited to distributions like Bernoulli, Beta, Chi2, Dirichlet, Gamma, Uniform, etc. They are important building blocks when it comes to build machine learning algorithms, especially for probabilistic approaches like Bayesian models.\n",
    "\n",
    "### Loss Functions and Metrics\n",
    "\n",
    "Machine learning algorithms rely on optimizations based the loss function provided. TensorFlow provides a wide range of loss functions to choose, such as sigmoid and softmax cross entropy, log-loss, hinge loss, sum of squares, sum of pairwise squares, etc.  \n",
    "\n",
    "A variety types of metrics are available, such as precision, recall, accuracy, auc, MSE, as well as their streaming versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regression'></a>\n",
    "\n",
    "## TensorFlow - Simple linear regression\n",
    "We'll try to produce the same model as with Sklearn but now using Tensorflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# slope\n",
    "a = 2\n",
    "# intercept\n",
    "b = -1\n",
    "rng = np.random.RandomState(42)\n",
    "x = 10 * rng.rand(50)\n",
    "y = a * x + b + rng.randn(50)\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# create model\n",
    "feat_cols = [tf.feature_column.numeric_column('x',shape=[1])]\n",
    "model = tf.estimator.LinearRegressor(feature_columns=feat_cols)\n",
    "# create input function for training\n",
    "train_input_func = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
    "    {'x': x}, y, batch_size=4, num_epochs=None, shuffle=True)\n",
    "# training (fit in sklearn)\n",
    "model.train(input_fn = train_input_func, steps = 1000)\n",
    "# create input function for predictions\n",
    "predict_input_func = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
    "    {'x':np.linspace(-1,11,8)},shuffle=False)\n",
    "# create predictions list (from model.predict)\n",
    "predictions = []\n",
    "for i in model.predict(input_fn = predict_input_func):\n",
    "    predictions.append(i['predictions'])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(np.linspace(-1,11,8), predictions,'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks more complicated to build a simple regression with Tensorflow than with Sklearn.  \n",
    "But what about Deep Learning models?\n",
    "\n",
    "<a id='tf-mlp'></a>\n",
    "\n",
    "## MLP Classifier: Handwritten Digits\n",
    "Back for our favorite test database MNIST for handwritten digits recognition. It is the same database we used in the example for MLP classification implemented in Sklearn. Somehow it is the ``Hello World`` example for simple image recognition in DNNs.  \n",
    "\n",
    "Even if Keras contains this database, we will use the same code and database as prepared for sklearn model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "# Converting data from (0-255) integer values to (0-1) float, a rescaling\n",
    "X = X / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train partition and test partition\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.5)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As extra, One Hot Encode for label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.utils import np_utils\n",
    "\n",
    "print('y_test before:\\n', y_test[0:3])\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "print('y_test after:\\n', y_test[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "- One single hidden layer with 40 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = keras.Sequential([\n",
    "    layers.Dense(40, input_dim=784, kernel_initializer='normal', activation='relu'),\n",
    "    layers.Dense(10, kernel_initializer='normal', activation='softmax')])\n",
    "mlp.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(mlp.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = mlp.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=30,\n",
    "    batch_size=256,\n",
    "    verbose=0,\n",
    "    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and Plot Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_history = pd.DataFrame(history.history)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "cut_first_n = 0\n",
    "df_history.loc[cut_first_n:, ['loss', 'val_loss']].plot(ax=ax[0])\n",
    "df_history.loc[cut_first_n:, ['accuracy', 'val_accuracy']].plot(ax=ax[1])\n",
    "ax[0].grid(which='both')\n",
    "ax[1].grid(which='both')\n",
    "plt.show()\n",
    "\n",
    "scores = mlp.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"MLP Error: {100-scores[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_test[0]\n",
    "plt.imshow(x.reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
    "for label, proba in enumerate(mlp.predict(x.reshape(1, -1))[0]):\n",
    "  print(f\"{label} with probability of {proba*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "N = 10\n",
    "fig, ax = plt.subplots(1, N, figsize=(16, 4))\n",
    "predictions = []\n",
    "predictions_p = []\n",
    "for _ in range(N):\n",
    "  i = random.randint(0, X_test.shape[0])\n",
    "  ax[len(predictions)].imshow(X_test[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
    "  res = mlp.predict(X_test[i].reshape(1, -1))\n",
    "  i = np.argmax(res)\n",
    "  predictions.append(np.argmax(res))\n",
    "  predictions_p.append(res[0][i])\n",
    "print(predictions)\n",
    "print(predictions_p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tf-cnn'></a>\n",
    "\n",
    "## Convolutional Neural Network (CNN)\n",
    "A Convolutional Neural Network, also known as ``CNN`` or ``ConvNet``, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image. A digital image is a binary representation of visual data.  \n",
    "Specialized DNNs, like this one, are not available as models in Sklearn.\n",
    "\n",
    "Let's try one for the same database, MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('Train: ', X_train.shape, y_train.shape)\n",
    "print('Test: ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape & Rescale\n",
    "In Keras, when we use the TensorFlow backend, the layers used for two-dimensional convolutions expect pixel values with the dimensions [width][height][channels]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "\n",
    "print('Train: ', X_train.shape, y_train.shape)\n",
    "print('Test: ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same One Hot Encoding the Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "\n",
    "1. Convolutional layer with 32 feature maps, from 5 x 5 filters and ReLu activation function. This is the input layer, expecting images with the structure outline above, 28 x 28 x 1.\n",
    "\n",
    "2. Pooling layer taking the max over 2 x 2 patches.\n",
    "\n",
    "3. Dropout layer with a probability of 20%.\n",
    "\n",
    "4. Flatten layer.\n",
    "\n",
    "5. Fully connected layer with 128 neurons and rectifier activation function.\n",
    "\n",
    "6. Output layer with 10 neurons for the 10 classes and a softmax activation function to output probability-like predictions for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = keras.Sequential([\n",
    "    layers.Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')])\n",
    "cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(cnn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "Same as before, 30 epochs.  \n",
    "It will take a while to complete the training since the CNN model is far more complex (params: 592,074) than MLP model (params: 31,810)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = cnn.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=30,\n",
    "    batch_size=256,\n",
    "    verbose=0,\n",
    "    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and Plot the Model Performance\n",
    "Using an almost identic code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history = pd.DataFrame(history.history)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "cut_first_n = 0\n",
    "df_history.loc[cut_first_n:, ['loss', 'val_loss']].plot(ax=ax[0])\n",
    "df_history.loc[cut_first_n:, ['accuracy', 'val_accuracy']].plot(ax=ax[1])\n",
    "ax[0].grid(which='both')\n",
    "ax[1].grid(which='both')\n",
    "plt.show()\n",
    "\n",
    "scores = cnn.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the performances are superior!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_test[0]\n",
    "plt.imshow(x, cmap=plt.get_cmap('gray'))\n",
    "for label, proba in enumerate(cnn.predict(x.reshape(1, 28, 28, 1))[0]):\n",
    "    print(f\"{label} with probability of {proba*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "fig, ax = plt.subplots(1, N, figsize=(16, 4))\n",
    "predictions = []\n",
    "predictions_p = []\n",
    "for _ in range(N):\n",
    "  i = random.randint(0, X_test.shape[0])\n",
    "  ax[len(predictions)].imshow(X_test[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
    "  res = cnn.predict(X_test[i].reshape(1, 28, 28, 1))\n",
    "  i = np.argmax(res)\n",
    "  predictions.append(np.argmax(res))\n",
    "  predictions_p.append(res[0][i])\n",
    "print(predictions)\n",
    "print(predictions_p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pt'></a>\n",
    "\n",
    "## PyTorch\n",
    "``PyTorch`` is an ``optimized tensor library`` primarily used for ``Deep Learning applications`` using GPUs and CPUs. It is an open-source machine learning library for Python, mainly developed by the ``Facebook`` AI Research team.  \n",
    "The PyTorch framework supports over 200 different mathematical operations. The popularity of PyTorch continues to rise as it simplifies the creation of artificial neural network (ANN) models. PyTorch is mainly used for applications of research, data science and artificial intelligence (AI).  \n",
    "Even if it is still a young framework it has a stronger community movement and it's more Python friendly (``pythonic``) than Tensorflow.  \n",
    "``Tesla`` utilizes Pytorch for distributed CNN training. For autopilot, Tesla trains around 48 networks that do 1,000 different predictions and it takes 70,000 GPU hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN: Handwritten Digits\n",
    "Now, we'll repeat the previous example written in Tensorflow/Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "\n",
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    tv.datasets.MNIST('/files/', train=True, download=True,\n",
    "            transform=tv.transforms.Compose([\n",
    "                    tv.transforms.ToTensor(),\n",
    "                    tv.transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "    batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    tv.datasets.MNIST('/files/', train=False, download=True,\n",
    "            transform=tv.transforms.Compose([\n",
    "                    tv.transforms.ToTensor(),\n",
    "                    tv.transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "    batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "example_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Little bit of visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]), color='white')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the CNN model\n",
    "We'll use two 2-D convolutional layers followed by two fully-connected (or linear) layers. As activation function we'll choose rectified linear units (ReLUs in short) and as a means of regularization we'll use two dropout layers. In PyTorch a nice way to build a network is by creating a new class for the network we wish to build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize the network and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "We'll keep track of the progress with some printouts. In order to create a nice training curve later on we also create two lists for saving training and testing losses. On the x-axis we want to display the number of training examples the network has seen during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to build our training loop. First we want to make sure the network is in ``training mode``. Then we iterate over all training data once per epoch. Loading the individual batches is handled by the DataLoader. First we need to manually set the gradients to zero using optimizer.zero_grad() since PyTorch by default accumulates gradients. We then produce the output of our network (forward pass) and compute a negative log-likelihodd loss between the output and the ground truth label. The backward() call we now collect a new set of gradients which we propagate back into each of the network's parameters using optimizer.step()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "      torch.save(network.state_dict(), 'results/model.pth')\n",
    "      torch.save(optimizer.state_dict(), 'results/optimizer.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for our test loop. Here we sum up the test loss and keep track of correctly classified digits to compute the accuracy of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to run the training! We'll manually add a test() call before we loop over n_epochs to evaluate our model with randomly initialized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model's Performance\n",
    "And that's it. With just 3 epochs of training we already managed to achieve 97% accuracy on the test set! We started out with randomly initialized parameters and as expected only got about 10% accuracy on the test set before starting the training.\n",
    "\n",
    "Let's plot the training curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    xm = random.randint(1, len(example_data))\n",
    "    plt.imshow(example_data[xm][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[xm]), color='white')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    with torch.no_grad():\n",
    "        test_output = network(example_data[xm])\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        print(f'Prediction number: {pred_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transfer'></a>\n",
    "\n",
    "## Transfer Learning\n",
    "So, the data needed to train a DNN must be very large, the models are complicated and will cost me a lot of time and money, then I cannot use it even if the results are remarcable?  \n",
    "\n",
    "Actually it may be a workaround:  \n",
    "\n",
    "The intuition behind transfer learning for, let's say, image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset.\n",
    "\n",
    "<span style=''><img style='background: rgb(128, 128, 128, .15); align: left; display: inline-block; padding: 20px' src='img/transfer.webp'/></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use transfer learning?\n",
    "Assuming you have 100 images of cats and 100 dogs and want to build a model to classify the images. How would you train a model using this small dataset? You can train your model from scratch, but it will most likely overfit horribly. Enter transfer learning. Generally speaking, there are two big reasons why you want to use transfer learning:\n",
    "- training models with high accuracy requires a lot of data. For example, the ImageNet dataset contains over 1 million images. In the real world, you are unlikely to have such a large dataset. \n",
    "- assuming that you had that kind of dataset, you might still not have the resources required to train a model on such a large dataset. Hence transfer learning makes a lot of sense if you donâ€™t have the compute resources needed to train models on huge datasets. \n",
    "- even if you had the compute resources at your disposal, you still have to wait for days or weeks to train such a model. Therefore using a pre-trained model will save you precious time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to implement transfer learning?\n",
    "You can implement transfer learning in these general steps. (with example in Keras)\n",
    "1. Obtain the pre-trained model  \n",
    "You can also optionally download the pre-trained weights. If you donâ€™t download the weights, you will have to use the architecture to train your model from scratch.\n",
    "- There are more than two dozen pre-trained models available from Keras. Theyâ€™re served via [Keras applications](https://keras.io/api/applications/). For instance, here is how you can initialize the MobileNet architecture trained on ImageNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading MobileNet model trained on imagenet as keras application\n",
    "model = keras.applications.MobileNet(\n",
    "    input_shape=None,\n",
    "    alpha=1.0,\n",
    "    depth_multiplier=1,\n",
    "    dropout=0.001,\n",
    "    include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    classes=1000,\n",
    "    classifier_activation=\"softmax\",\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Itâ€™s worth mentioning that Keras applications are not your only option for transfer learning tasks. You can also use models from [TensorFlow Hub](https://www.tensorflow.org/hub):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tensor flow hub to create a model, see the output level added (let's say cats and dogs)\n",
    "import tensorflow_hub as hub\n",
    "model = keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/5\", trainable=False), \n",
    "    keras.layers.Dense(2, activation='softmax')])\n",
    "model.build([None, 224, 224, 3])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a model\n",
    "Based on the pretrained model create your own model and remove the final output level. Later on, you will add a final output layer that is compatible with your problem.\n",
    "3. Freeze layers so they donâ€™t change during training\n",
    "4. Add new trainable layers  \n",
    "The next step is to add new trainable layers that will turn old features into predictions on the new dataset. This is important because the pre-trained model is loaded without the final output layer.\n",
    "5. Train the new layers on the dataset\n",
    "6. Improve the model via fine-tuning  \n",
    "Fine-tuning is done by unfreezing the base model or part of it and training the entire model again on the whole dataset at a very low learning rate. The low learning rate will increase the performance of the model on the new dataset while preventing overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "<span style='background: rgb(128, 128, 128, .15); width: 100%; display: block; padding: 10px 0 10px 10px'>< [Scikit-learn](06.01-Scikit.ipynb) | [Contents](00.00-Index.ipynb) | [Quiz](06.03-Quiz.ipynb) > [Top](#top) ^ </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background: rgb(128, 128, 128, .15); width: 100%; display: block; padding: 10px 0 10px 10px'>This is the Jupyter notebook version of the __Python for Official Statistics__ produced by Eurostat; the content is available [on GitHub](https://github.com/eurostat/e-learning/tree/main/python-official-statistics).\n",
    "<br>The text and code are released under the [EUPL-1.2 license](https://github.com/eurostat/e-learning/blob/main/LICENSE).</span>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "12d738f96725d1c1f433a1d40c5369c2dd6b861cec3a8aa29acd662c91ac2528"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

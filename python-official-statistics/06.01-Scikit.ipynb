{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "<span style='background: rgb(128, 128, 128, .15); width: 100%; display: block; padding: 10px 0 10px 10px'>< [Quiz](05.03-Quiz.ipynb) | [Contents](00.00-Index.ipynb) | [Tensorflow + Keras and PyTorch](06.02-Tensorflow.ipynb) ></span>\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/eurostat/e-learning/blob/main/python-official-statistics/06.01-Scikit.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "# Scikit-learn\n",
    "## Content  \n",
    "- [Categories of Machine Learning](#categories)\n",
    "- [Scikit-learn API](#API)\n",
    "- [Simple linear regression - Supervised](#regression)\n",
    "- [Iris classification - Supervised](#classification)\n",
    "- [Iris dimensionality reduction - Unsupervised](#dimensionality)\n",
    "- [Iris clustering - Unsupervised](#clustering)\n",
    "- [Deep learning](#neuralnet)\n",
    "\n",
    "Machine learning (``ML``) is a type of artificial intelligence (``AI``) that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. Machine learning algorithms use historical data as input to predict new output values.  \n",
    "Deep learning (``DL``) is a subset of ML based on neural networks.\n",
    "\n",
    "<span style=''><img style='background: rgb(128, 128, 128, .15); align: left; display: inline-block; padding: 20px' src='img/ai.webp'/></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='categories'></a>\n",
    "\n",
    "## Categories of Machine Learning\n",
    "### Supervised learning\n",
    "Involves modeling the relationship between measured features of data and some label associated with the data; once this model is determined, it can be used to apply labels to new, unknown data.\n",
    "This is subdivided into:\n",
    "- Classification: The labels are discrete categories.\n",
    "- Regression: The labels are continuous quantities.  \n",
    "\n",
    "### Unsupervised learning\n",
    "Involves modeling the features of a dataset without reference to any label, and is often described as \"letting the dataset speak for itself.\"\n",
    "These models include:\n",
    "- Clustering: Algorithms to identify distinct groups of data.\n",
    "- Dimensionality reduction: Searching for more succinct representations of the data.  \n",
    "\n",
    "### Semi-supervised learning methods\n",
    "They falls somewhere between supervised learning and unsupervised learning.\n",
    "Semi-supervised learning methods are often useful when only incomplete labels are available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='API'></a>\n",
    "\n",
    "## Scikit-learn API\n",
    "Scikit-learn is an increasingly popular machine learning library. It is designed to be simple and efficient, accessible to non-experts, and reusable in various contexts.  \n",
    "In practice it makes Scikit-Learn very easy to use, once the basic principles are understood.\n",
    "Every machine learning algorithm in Scikit-Learn is implemented via the Estimator API, which provides a consistent interface for a wide range of machine learning applications.  \n",
    "\n",
    "### Basics of the API\n",
    "Most commonly, the steps in using the Scikit-Learn estimator API are as follows:\n",
    "- Choose a class of model by importing the appropriate estimator class from Scikit-Learn.\n",
    "- Choose model hyperparameters by instantiating this class with desired values.\n",
    "- Prepare training data as per model requirements.\n",
    "- Train (fit) the model with your data by calling the ``fit()`` method of the model instance.\n",
    "- Apply the Model to new data:\n",
    "   - For supervised learning the prediction is done using the ``predict()`` method.\n",
    "   - For unsupervised learning there are ``transform()`` or ``predict()`` methods.\n",
    "\n",
    "We will step now through several examples of applying supervised and unsupervised learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regression'></a>\n",
    "\n",
    "## Supervised learning example: Simple linear regression\n",
    "\n",
    "As an example of this process, let's consider a simple linear regression—that is, the common case of fitting a line to $(x, y)$ data.\n",
    "We will use some random data for our regression example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# slope\n",
    "a = 2\n",
    "# intercept\n",
    "b = -1\n",
    "rng = np.random.RandomState(42)\n",
    "x = 10 * rng.rand(50)\n",
    "y = a * x + b + rng.randn(50)\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data in place, we can use the process shown earlier:\n",
    "### - Choose a class of model\n",
    "\n",
    "In Scikit-Learn, every class of model is represented by a Python class.\n",
    "So, for example, if we would like to compute a simple linear regression model, we can import the linear regression class (there are many [``linear models``](http://Scikit-Learn.org/stable/modules/linear_model.html) implemented in Scikit-Learn):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Choose model hyperparameters\n",
    "Once we have decided on our model class, there are still some options open to us.\n",
    "Depending on the model class we are working with, we need to answer some questions to configure the model.\n",
    "These choices are often represented as *hyperparameters*, or parameters that must be set before the model is fit to data.\n",
    "In Scikit-Learn, hyperparameters are chosen by passing values at model instantiation. For our example we will use the default values, for more we recommend you study how to configure the training of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Prepare training data\n",
    "Arrange data into a ``features matrix`` and ``target vector``.  \n",
    "#### Features matrix\n",
    "Represent an array of ``samples``, for the same type of objects, having ``features`` that describe each sample in a quantitative manner.  \n",
    "Features are generally real values, but may be Boolean or discrete-valued in some cases.\n",
    "#### Target array\n",
    "In addition to the feature matrix, we also generally work with a *label* or *target* array.\n",
    "The target array is usually one dimensional.  \n",
    "The target array may have continuous numerical values, or discrete classes/labels.\n",
    "\n",
    "The Scikit-Learn data representation, which requires a two-dimensional features matrix, usually called ``X``, and a one-dimensional target array, ``y``.  \n",
    "\n",
    "![](img/features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target variable ``y`` is already in the correct form (a length-``n_samples`` array), but we need to reshape the data ``x`` to make it a matrix of size ``[n_samples, n_features]``.\n",
    "In our case ``n_features = 1``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x shape before transformation \n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X (capital) with a new dimmension added: now a matrix\n",
    "X = x[:, np.newaxis]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Fit the model to your data\n",
    "Now it is time to apply our model to data.\n",
    "This can be done with the ``fit()`` method of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ``fit()`` command causes a number of model-dependent internal computations to take place, and the results of these computations are stored in model-specific attributes that the user can explore.\n",
    "In Scikit-Learn, by convention all model parameters that were learned during the ``fit()`` process have trailing underscores; for example in this linear model, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two parameters represent the slope and intercept of the simple linear fit to the data.\n",
    "Comparing to the data definition, we see that they are very close to the input slope of 2 and intercept of -1.\n",
    "### - Predict labels for unknown data\n",
    "Once the model is trained, the main task of supervised machine learning is to evaluate it based on what it says about new data that was not part of the training set.\n",
    "In Scikit-Learn, this can be done using the ``predict()`` method.\n",
    "For the sake of this example, our \"new data\" will be a grid of *x* values, and we will ask what *y* values the model predicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 11, 8)\n",
    "Xfit = xfit[:, np.newaxis]\n",
    "yfit = model.predict(Xfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's visualize the results by plotting the raw (training) data and the model fit (estimates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='classification'></a>\n",
    "\n",
    "## Supervised learning example: Iris classification\n",
    "\n",
    "Let's take a look at another example of this process, using the Iris dataset we discussed earlier.\n",
    "Our question will be this: given a model trained on a portion of the Iris data, how well can we predict the remaining labels?\n",
    "\n",
    "For this task, we will use an extremely simple model known as [Gaussian naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB), which proceeds by assuming each class is drawn from an axis-aligned Gaussian distribution.\n",
    "Because it is so fast and has no hyperparameters to choose, Gaussian naive Bayes is often a good model to use as a baseline classification, before exploring whether improvements can be found through more sophisticated models.\n",
    "\n",
    "We would like to evaluate the model on data it has not seen before, and so we will split the data into a *training set* and a *testing set*.\n",
    "This could be done by hand, but it is more convenient to use the ``train_test_split`` utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')\n",
    "print(iris.sample(5))\n",
    "print('\\noriginal data shape:', iris.shape)\n",
    "X_iris = iris.drop('species', axis=1)\n",
    "y_iris = iris['species']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# if train_size is None (implicit), it will be set to 0.75.\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris, random_state=1)\n",
    "print('\\n', Xtest.sample(3), '\\n')\n",
    "print(ytest.sample(3))\n",
    "Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data prepared, we can follow our process to predict the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Choose model class\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Instantiate model (no hyperparameters)\n",
    "model = GaussianNB()\n",
    "# Fit model to data\n",
    "model.fit(Xtrain, ytrain)\n",
    "# Predict on new data\n",
    "y_model = model.predict(Xtest)\n",
    "print(ytest.sample(3), '\\n')\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we can use the ``accuracy_score`` utility to see the fraction of predicted labels that match their correct value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(f'Effectivness of the predictions: {100*accuracy_score(ytest, y_model):5.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dimensionality'></a>\n",
    "\n",
    "## Unsupervised learning example: Iris dimensionality\n",
    "As an example of an unsupervised learning problem, let's take a look at reducing the dimensionality of the Iris data (from 4 to 2) so as to more easily visualize it.\n",
    "\n",
    "The task of dimensionality reduction is to ask whether there is a suitable lower-dimensional representation that retains the essential features of the data.\n",
    "Often dimensionality reduction is used as an aid to visualizing data: after all, it is much easier to plot data in two dimensions than in four dimensions or higher!\n",
    "\n",
    "Here we will use principal component analysis (PCA), which is a fast linear dimensionality reduction technique.\n",
    "We will ask the model to return two components—that is, a two-dimensional representation of the data.\n",
    "\n",
    "Following the sequence of steps outlined earlier, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model class\n",
    "from sklearn.decomposition import PCA\n",
    "# Instantiate the model with hyperparameters\n",
    "model = PCA(n_components=2)\n",
    " # Fit to data. Notice y is not specified!\n",
    "model.fit(X_iris)\n",
    "# Transform the data to two dimensions\n",
    "X_2D = model.transform(X_iris)\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['PCA1'] = X_2D[:, 0]\n",
    "iris['PCA2'] = X_2D[:, 1]\n",
    "sns.lmplot(x=\"PCA1\", y=\"PCA2\", hue='species', data=iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still in the two-dimensional representation, the species are well separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clustering'></a>\n",
    "\n",
    "## Another unsupervised learning: Iris clustering\n",
    "\n",
    "Let's now try to apply clustering to the Iris data.\n",
    "A clustering algorithm attempts to find distinct groups of data without reference to any labels.\n",
    "Here we will use a powerful clustering method called a [Gaussian mixture model](https://scikit-learn.org/stable/modules/mixture.html#gaussian-mixture) (GMM).\n",
    "A GMM attempts to model the data as a collection of Gaussian blobs.\n",
    "\n",
    "We can fit the Gaussian mixture model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model class\n",
    "from sklearn.mixture import GaussianMixture\n",
    "# Instantiate the model with hyperparameters\n",
    "model = GaussianMixture(n_components=3, covariance_type='full')\n",
    "# Fit to data. Notice y is not specified!\n",
    "model.fit(X_iris)\n",
    "# Determine cluster labels\n",
    "y_gmm = model.predict(X_iris)\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we are using Seaborn to plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['cluster'] = y_gmm\n",
    "sns.lmplot(x=\"PCA1\", y=\"PCA2\", data=iris, hue='species', col='cluster', fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of algorithm might give experts in the field clues as to the relationship between the samples they are observing.\n",
    "\n",
    "<a id='neuralnet'></a>\n",
    "\n",
    "## Deep learning\n",
    "Deep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks.  \n",
    "With DL models, practically, we'll be able to approach all the same kind of problems as solved by other ML models, like regression, classification, or unsupervised - clustering etc.  \n",
    "So why treating it separately? There are several motives but most of them can be summarized as economics:\n",
    "### Large amount of training data\n",
    "For majority of ``old ML models`` after some amount the training data the performance will ``plateau``, but for ``DL models`` is keep ``geting better`` the more data is added.\n",
    "### Large models\n",
    "With neural networks, in general, the more layers and ``neurons`` you add the more performance you can obtain. This is why for DL the so called ``hierarchical feature learning`` when DL models automatically do feature extraction from row data, so the model become more important than understanding the data, learning without depending completely on human-crafted features.  \n",
    "### More computation\n",
    "As a result of both large training data and large models, the duration of training increase and the training price for such a model become quite expensive.\n",
    "Sometimes it is reffered as ``scalability of neural networks`` indicating that results get better with more data and larger models, that in turn require more computation to train.\n",
    "### GPT-3: The Largest Neural Network Ever Created\n",
    "As an example of achieved magnitudes in DL models let's explore GPT-3. OpenAI's GPT-3 architecture represents a seminal shift in AI research and use. The largest neural network ever developed promises significant improvements in ``natural language`` tools and applications.\n",
    "- 96 layers and 175 billion parameters: Parameters in machine language parlance depict skills or knowledge of the model, so the higher the number of parameters, the more skillful the model is. (100x bigger than GPT-2)\n",
    "- 45 terabytes of data: Training data from CommonCrawl, WebText, Wikipedia, and a corpus of books.\n",
    "- Cost and time: Since this information is not public, according to some estimate, training the 175-billion-parameter neural network requires 3.114E23 FLOPS (floating-point operation), which would theoretically take 355 years on a V100 GPU server with 28 TFLOPS capacity and would cost 4.6 million at $1.5 per hour.\n",
    "### When And Where To Apply Deep Learning?\n",
    "There’s no denying that deep learning is a hot topic right now. But what does it really mean, and how should it be applied in practise?  \n",
    "- ``When not to use deep learning``  \n",
    "For one thing, deep learning really ``needs Big Data`` to make accurate decisions. So if you haven’t got an extremely large dataset to learn from, a regular machine learning algorithm is likely to deliver more accurate results.\n",
    "It’s also ``more expensive`` to implement because it takes ``a lot of computing power`` to run a deep learning network. While services and tools like IBM’s Watson are helping to lower the barrier to entry for deep learning, remember that deep learning is still at the very cutting edge.\n",
    "\n",
    "- ``Where best to apply deep learning``  \n",
    "Deep learning is ideal for predicting outcomes whenever you have a lot of data to learn from – ‘a lot’ being a huge dataset with hundreds of thousands or better ``millions of data points``. Where you have a huge volume of data like this, the system has what it needs to train itself.\n",
    "It’s also best when applied to ``complex problems`` and things that would be vastly expensive to solve with human decision making. ``Image processing`` is a great example of this. So, rather than YouTube paying an army of human workers to trawl through millions of videos and tag the ones with cats for our viewing pleasure, it makes much more sense to apply deep learning. It’s the same with ``translation`` and ``speech recognition``.\n",
    "And last but not least, deep learning is only appropriate if you have the high-end computing power to make it work, or are partnering with an analytics provider who has the infrastructure and skills that might be lacking in-house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn and DNN\n",
    "``Sklearn`` doesn't have much support for ``Deep Neural Networks``, but can be a good point to start exploring the realm of DL. More like a research approach since it is very easy to create and train a model. The implementation is not intended for large-scale applications. In particular, scikit-learn offers no GPU support. \n",
    "\n",
    "### Example: Handwritten Digit Recognition\n",
    "- The dataset was constructed from a number of scanned document datasets available from the _National Institute of Standards and Technology (NIST)_. This is where the name for the dataset\n",
    "comes from, as the _Modified NIST_ or [_MNIST_ dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "- Images of digits were taken from a variety of scanned documents, normalized in size and centered. This makes it an excellent dataset for evaluating models, allowing the developer to focus on the machine learning with very little data cleaning or preparation required.\n",
    "- Each image is a 28 x 28 pixel square (784 pixels total). A standard split of the dataset is used to evaluate and compare models, where 60'000 images are used to train a model and a separate set of 10'000 images are used to test it.\n",
    "- It is a digit recognition task. As such there are 10 digits (0 to 9) or 10 classes to predict.  \n",
    "\n",
    "<img src=\"img/mnist.png\"\n",
    "alt=\"drawing\" width=\"700\"/>  \n",
    "\n",
    "To make the example run faster, we use very few hidden units, and train only for a very short time. Training longer would result in weights with a much smoother spatial appearance.  \n",
    "\n",
    "- First, let's fetch and partition the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "# Converting data from (0-255) integer values to (0-1) float, a rescaling\n",
    "X = X / 255.0\n",
    "\n",
    "# Split data into train partition and test partition\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.5)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some visualization and analysis for training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number: {y[0]}')\n",
    "plt.imshow(X[0].reshape(28,28), cmap='gray')\n",
    "plt.show()\n",
    "X[0].min(), X[0].mean(), X[0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-layer Perceptron classifier (``MLP``) is the class offered as Neural network model (supervised) for classification by Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(40,),\n",
    "    max_iter=30,\n",
    "    alpha=1e-4,\n",
    "    solver=\"sgd\",\n",
    "    verbose=10,\n",
    "    random_state=1,\n",
    "    learning_rate_init=0.2,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=.2\n",
    ")\n",
    "# Train the model\n",
    "mlp.fit(X_train, y_train)\n",
    "# print(mlp.predict(X_test))\n",
    "print(f\"\\nTraining set score: {mlp.score(X_train, y_train):.3f}\")\n",
    "print(f\"Test set score: {mlp.score(X_test, y_test):.3f}\")\n",
    "\n",
    "mlp.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_left = plt.subplots()\n",
    "ax_right = ax_left.twinx()\n",
    "ax_left.set_ylabel('training loss', color='green')\n",
    "ax_left.plot(mlp.loss_curve_, color='green')\n",
    "ax_right.set_ylabel('validation score', color='orange')\n",
    "ax_right.plot(mlp.validation_scores_, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "fig, axes = plt.subplots(4, 10)\n",
    "# use global min / max to ensure all weights are shown on the same scale\n",
    "vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n",
    "for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see all 40 groups of parameter's values for the entries of the hidden level. Shown as 28X28 images, they tell us how, after training, the initial image is filtered by each ``arificial neuron`` to make sense of input data.  \n",
    "And, even if in this representation looks like it make graphical sense: like trying to learn horizontal, vertical or diagonal patterns; don't forget that for the model the input is just a row of 784 floating point values between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "<span style='background: rgb(128, 128, 128, .15); width: 100%; display: block; padding: 10px 0 10px 10px'>< [Quiz](05.03-Quiz.ipynb) | [Contents](00.00-Index.ipynb) | [Tensorflow + Keras and PyTorch](06.02-Tensorflow.ipynb) > [Top](#top) ^ </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background: rgb(128, 128, 128, .15); width: 100%; display: block; padding: 10px 0 10px 10px'>This is the Jupyter notebook version of the __Python for Official Statistics__ produced by Eurostat; the content is available [on GitHub](https://github.com/eurostat/e-learning/tree/main/python-official-statistics).\n",
    "<br>The text and code are released under the [EUPL-1.2 license](https://github.com/eurostat/e-learning/blob/main/LICENSE).</span>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "12d738f96725d1c1f433a1d40c5369c2dd6b861cec3a8aa29acd662c91ac2528"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

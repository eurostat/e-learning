{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "<span style='background: rgb(128, 128, 128, .15); width: 100%; display: block; padding: 10px 0 10px 10px'>< [Series & Dataframes](04.01-Dataframes.ipynb) | [Contents](00.00-Index.ipynb) | [Quiz 4](04.03-Quiz.ipynb) ></span>\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/eurostat/e-learning/blob/main/python-official-statistics/04.02-Pandas.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "# Pandas: Advanced Processing\n",
    "## Content  \n",
    "- [Load from sources (and save to)](#load)\n",
    "- [Handling missing data](#missing)\n",
    "- [Dealing with duplicates](#duplicates)\n",
    "- [Hierarchical Indexing](#hierarchical)\n",
    "- [Combining Datasets: Concat, Append, Merge and Join](#combining)\n",
    "- [Aggregation and Grouping](#aggregation)\n",
    "- [Pivot Tables](#pivot)\n",
    "- [Time Series](#time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load'></a>\n",
    " \n",
    "## Load from sources (and save to)\n",
    "Usually data to process comes from all kind of sources in a variety of formats.\n",
    "Pandas provides helper functions to read data from various file formats like CSV, Excel spreadsheets, HTML tables, JSON, SQL, and more. And the same capabilities for saving dataframes to different formats. We'll use a little bit of Jupyter magic to help you find them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# magic for read functions\n",
    "?pd.read_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic for save functions\n",
    "A = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\n",
    "?A.to_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For exemplification we saved an Eurostat database `hlth_ehis_pe9e.tsv.gz` in our local folder data. This file is TAB separated and also archived, but not a problem for Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "health = pd.read_csv('data/hlth_ehis_pe9e.tsv.gz', sep='\\t', index_col=0)\n",
    "print(health.describe())\n",
    "# play with column names (because of spaces)\n",
    "print(health.columns)\n",
    "health.columns = map(str.strip, list(health.columns.array))\n",
    "print(health.columns)\n",
    "\n",
    "print(health.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will save the column 'BE' as a table into a SQLite database. As simple as two lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "with sqlite3.connect('data/test.db') as conn:\n",
    "    health.to_sql('BE', conn, if_exists='replace')\n",
    "# health.to_csv('data/test.csv.zip', compression='zip')\n",
    "# health2 = pd.read_csv('data/test.csv.zip', index_col=0)\n",
    "# print(health2.info())\n",
    "# print(health.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='missing'></a>\n",
    " \n",
    "## Handling missing data\n",
    "What does \"missing data\" mean? What is a missing value? It depends on the origin of the data and the context it was generated. For example, for a survey, a _`Salary`_ field with an empty value, or a number 0, or an invalid value (a string for example) can be considered \"missing data\". These concepts are related to the values that Python will consider \"Falsy\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falsy_values = (0, False, None, '', [], {})\n",
    "any(falsy_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy has a special \"nullable\" value for numbers which is `np.nan`. It's _NaN_: \"Not a number\".  \n",
    "The `np.nan` value is kind of a virus. Everything that it touches becomes `np.nan`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(3 + np.nan)\n",
    "a = np.array([1, 2, 3, np.nan, np.inf, 4])\n",
    "print(a.sum())\n",
    "print(a.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy also supports an \"Infinite\" type: `np.inf`  \n",
    "Which also behaves as a virus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(3 + np.inf)\n",
    "print(np.inf / 3)\n",
    "print(np.inf / np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First example of processing missing data, for out `health` DataFrame, is trying to convert data to numeric, but this is not always possible, and will end up in some `nan` values.  \n",
    "For a column (Series) the `to_numeric` function can be directly used. But this will not work for several columns or whole DataFrame. In this case we'll `apply` the function `to_numeric` for our columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to float (handling the missing data)\n",
    "# entyre dataframe\n",
    "print(health.apply(pd.to_numeric, errors='coerce').head())\n",
    "# selection of columns\n",
    "print(health[['BE', 'RO']].apply(pd.to_numeric, errors='coerce').head())\n",
    "# one column\n",
    "print(pd.to_numeric(health['BE'], errors='coerce').head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='duplicates'></a>\n",
    " \n",
    "## Dealing with duplicates\n",
    "Checking duplicate values is extremely simple. Itll behave differently between `Series` and `DataFrames`. Let's start with Series. As an example, let's say United States is throwing a fancy party and are inviting Ambassadors from Europe. But can only invite one ambassador per country. This is our original list, and as you can see, both the UK and Germany have duplicated ambassadors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambassadors = pd.Series([\n",
    "    'France',\n",
    "    'United Kingdom',\n",
    "    'United Kingdom',\n",
    "    'Italy',\n",
    "    'Germany',\n",
    "    'Germany',\n",
    "    'Germany',\n",
    "], index=[\n",
    "    'Gérard Araud',\n",
    "    'Kim Darroch',\n",
    "    'Peter Westmacott',\n",
    "    'Armando Varricchio',\n",
    "    'Peter Wittig',\n",
    "    'Peter Ammon',\n",
    "    'Klaus Scharioth '\n",
    "])\n",
    "ambassadors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two most important methods to deal with duplicates are `duplicated` (that will tell you which values are duplicates) and `drop_duplicates` (which will just get rid of duplicates):\n",
    "print(ambassadors.duplicated())\n",
    "# In this case `duplicated` didn't consider `'Kim Darroch'`, the first instance of the United Kingdom or `'Peter Wittig'` as duplicates. That's because, by default, it'll consider the first occurrence of the value as not-duplicate. You can change this behavior with the `keep` parameter:\n",
    "print(ambassadors.duplicated(keep='last'))\n",
    "# You can also choose to mark all of them as duplicates with `keep=False`:\n",
    "print(ambassadors.duplicated(keep=False))\n",
    "# A similar method is `drop_duplicates`, which just excludes the duplicated values and also accepts the `keep` parameter:\n",
    "print(ambassadors.drop_duplicates())\n",
    "print(ambassadors.drop_duplicates(keep='last'))\n",
    "print(ambassadors.drop_duplicates(keep=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates in DataFrames\n",
    "Conceptually speaking, duplicates in a DataFrame happen at \"row\" level. Two rows with exactly the same values are considered to be duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = pd.DataFrame({\n",
    "    'Name': [\n",
    "        'Kobe Bryant',\n",
    "        'LeBron James',\n",
    "        'Kobe Bryant',\n",
    "        'Carmelo Anthony',\n",
    "        'Kobe Bryant',\n",
    "    ],\n",
    "    'Pos': [\n",
    "        'SG',\n",
    "        'SF',\n",
    "        'SG',\n",
    "        'SF',\n",
    "        'SF'\n",
    "    ]\n",
    "})\n",
    "players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the previous DataFrame, we clearly see that Kobe is duplicated; but he appears with two different positions. What does `duplicated` say?\n",
    "players.duplicated()\n",
    "# Again, conceptually, \"duplicated\" means \"all the column values should be duplicates\". We can customize this with the `subset` parameter:\n",
    "players.duplicated(subset=['Name'])\n",
    "# And the same rules of `keep` still apply:\n",
    "players.duplicated(subset=['Name'], keep='last')\n",
    "# `drop_duplicates` takes the same parameters:\n",
    "players.drop_duplicates()\n",
    "players.drop_duplicates(subset=['Name'])\n",
    "players.drop_duplicates(subset=['Name'], keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hierarchical'></a>\n",
    " \n",
    "## Hierarchical Indexing\n",
    "Up to this point we've been focused primarily on one-dimensional and two-dimensional data, stored in Pandas ``Series`` and ``DataFrame`` objects. Often it is useful to go beyond this and store higher-dimensional data–that is, data indexed by more than one or two keys.  \n",
    "A common pattern in practice is to make use of *hierarchical indexing* (also known as *multi-indexing*) to incorporate multiple index *levels* within a single index.\n",
    "In this way, higher-dimensional data can be compactly represented within the familiar one-dimensional ``Series`` and two-dimensional ``DataFrame`` objects.  \n",
    "In a ``DataFrame``, the rows and columns are completely symmetric, and just as the rows can have multiple levels of indices, the columns can have multiple levels as well.  \n",
    "In our example the multi-index reffers to columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader.data as web\n",
    "df = web.DataReader('ttr00002', 'eurostat', start='2008')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.names)\n",
    "df = df.reorder_levels(['UNIT', 'FREQ', 'TRA_INFR', 'GEO'], axis=1, )\n",
    "print(df.columns.get_level_values(0).unique())\n",
    "print(df.columns.get_level_values(1).unique())\n",
    "print(df.columns.get_level_values(2).unique())\n",
    "df['Kilometre', 'Annual','Motorways']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='combining'></a>\n",
    " \n",
    "## Combining Datasets: Concat, Append, Merge and Join\n",
    "### pd.concat()\n",
    "Pandas has a function, ``pd.concat()``, which has a similar syntax to ``np.concatenate`` but contains a number of options that we'll discuss momentarily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for series\n",
    "ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3])\n",
    "ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6])\n",
    "print(ser1)\n",
    "print(ser2)\n",
    "print(pd.concat([ser1, ser2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some simple helpers to speed up the examples\n",
    "# Just a class used to display inline\n",
    "class display(object):\n",
    "\ttemplate = \"\"\"<div style=\"float: left; padding: 10px;\">\n",
    "\t<p style='font-family:\"Courier New\", Courier, monospace'>{0}</p>{1}\n",
    "\t</div>\"\"\"\n",
    "\tdef __init__(self, *args):\n",
    "\t\tself.args = args\n",
    "\tdef _repr_html_(self):\n",
    "\t\treturn '\\n'.join(self.template.format(a, eval(a)._repr_html_()) for a in self.args)\n",
    "\tdef __repr__(self):\n",
    "\t\tscope = locals()\n",
    "\t\treturn '\\n\\n'.join(a + '\\n' + repr(eval(a)) for a in self.args)\n",
    "# create simple dataframes\n",
    "def make_df(cols, ind):\n",
    "\t\"\"\"Quickly make a DataFrame\"\"\"\n",
    "\tdata = {c: [str(c) + str(i) for i in ind]\n",
    "\t\t\tfor c in cols}\n",
    "\treturn pd.DataFrame(data, ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple concatenation (by row)\n",
    "df1 = make_df('AB', [1, 2])\n",
    "df2 = make_df('AB', [3, 4])\n",
    "display('df1', 'df2', 'pd.concat([df1, df2])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple concatenation (by col)\n",
    "df3 = make_df('AB', [0, 1])\n",
    "df4 = make_df('CD', [0, 1])\n",
    "display('df3', 'df4', \"pd.concat([df3, df4], axis=1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate index if not a problem\n",
    "x = make_df('AB', [0, 1])\n",
    "y = make_df('AB', [2, 3])\n",
    "y.index = x.index  # make duplicate indices!\n",
    "display('x', 'y', 'pd.concat([x, y])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or verify integrity of the index\n",
    "try:\n",
    "    pd.concat([x, y], verify_integrity=True)\n",
    "except ValueError as e:\n",
    "    print(\"ValueError:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or just ignoring the index\n",
    "display('x', 'y', 'pd.concat([x, y], ignore_index=True)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or create a multi-index\n",
    "display('x', 'y', \"pd.concat([x, y], keys=['x', 'y'])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining for columns that differ (all)\n",
    "df5 = make_df('ABC', [1, 2])\n",
    "df6 = make_df('BCD', [3, 4])\n",
    "display('df5', 'df6', 'pd.concat([df5, df6])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining for common columns\n",
    "display('df5', 'df6', \"pd.concat([df5, df6], join='inner')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pd.merge & pd.join\n",
    "One essential feature offered by Pandas is its high-performance, in-memory join and merge operations.\n",
    "If you have ever worked with databases, you should be familiar with this type of data interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-to-one joins\n",
    "df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})\n",
    "df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],\n",
    "                    'hire_date': [2004, 2008, 2012, 2014]})\n",
    "df3 = pd.merge(df1, df2)\n",
    "display('df1', 'df2', 'pd.merge(df1, df2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many-to-one joins\n",
    "df4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'],\n",
    "                    'supervisor': ['Carly', 'Guido', 'Steve']})\n",
    "display('df3', 'df4', 'pd.merge(df3, df4)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many-to-many joins\n",
    "df5 = pd.DataFrame({'group': ['Accounting', 'Accounting',\n",
    "                              'Engineering', 'Engineering', 'HR', 'HR'],\n",
    "                    'skills': ['math', 'spreadsheets', 'coding', 'linux',\n",
    "                               'spreadsheets', 'organization']})\n",
    "display('df1', 'df5', \"pd.merge(df1, df5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same thing with join (but a key must be in place)\n",
    "df1.set_index('employee', inplace=True)\n",
    "df2.set_index('employee', inplace=True)\n",
    "display('df1', 'df2', 'df1.join(df2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aggregation'></a>\n",
    " \n",
    "## Aggregation and Grouping\n",
    "For a ``DataFrame``, by default the aggregates return results within each column, or row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "df = pd.DataFrame({'A': rng.rand(5),\n",
    "                   'B': rng.rand(5)})\n",
    "print(df)\n",
    "# by rows\n",
    "print(df.sum())\n",
    "# by columns\n",
    "print(df.sum(axis='columns'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple aggregations can give you a flavor of your dataset, but often we would prefer to aggregate conditionally on some label or index: this is implemented in the so-called ``groupby`` operation.\n",
    "The name \"group by\" comes from a command in the SQL database language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
    "                   'data': range(6)}, columns=['key', 'data'])\n",
    "display('df', 'df.groupby(\"key\").sum()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pivot'></a>\n",
    " \n",
    "## Pivot Tables\n",
    "The pivot table takes simple column-wise data as input, and groups the entries into a two-dimensional table that provides a multidimensional summarization of the data.\n",
    "The difference between pivot tables and ``GroupBy`` can sometimes cause confusion; it helps me to think of pivot tables as essentially a *multidimensional* version of ``GroupBy`` aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with groupby\n",
    "titanic.groupby(['sex', 'class'])['survived'].aggregate('mean').unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pivot table\n",
    "titanic.pivot_table('survived', index='sex', columns='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='time'></a>\n",
    " \n",
    "## Time Series\n",
    "Time series are very well represented in Pandas, and we just scratch the surface here. For more information you can refer to the [\"Time Series/Date\" section](http://pandas.pydata.org/pandas-docs/stable/timeseries.html) of the Pandas online documentation. All starts with the elementary data types used for indexes:\n",
    "### Python date and time\n",
    "Python's basic objects for working with dates and times reside in the built-in [datetime](https://docs.python.org/3/library/datetime.html) module.\n",
    "Along with the third-party [dateutil](http://labix.org/python-dateutil) module, you can use it to quickly perform a host of useful functionalities on dates and times.  \n",
    "The power of ``datetime`` and ``dateutil`` lie in their flexibility and easy syntax: you can use these objects and their built-in methods to easily perform nearly any operation you might be interested in.\n",
    "But they are not efficient to work with large arrays of dates and times:\n",
    "just as lists of Python numerical variables are suboptimal compared to NumPy-style typed numerical arrays, lists of Python datetime objects are suboptimal compared to typed arrays of encoded dates.\n",
    "### Typed arrays of times: NumPy's ``datetime64``\n",
    "With this NumPy type you can create uniform arrays, where processing can be accomplished much more quickly. The trade-off is the number of units encoded $2^{64}$.  \n",
    "For example, if you want a time resolution of one nanosecond, you only have enough information to encode a range of $2^{64}$ nanoseconds, or just under 600 years.\n",
    "NumPy will infer the desired unit from the input; for example, here is a day-based datetime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.datetime64('2015-07-04').dtype)\n",
    "print(np.datetime64('2015-07-04 12:00').dtype)\n",
    "print(np.datetime64('2015-07-04 12:59:59.50', 'ns').dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table, drawn from the [NumPy datetime64 documentation](http://docs.scipy.org/doc/numpy/reference/arrays.datetime.html), lists the available format codes along with the relative and absolute timespans that they can encode:\n",
    "\n",
    "|Code    | Meaning     | Time span (relative) | Time span (absolute)   |\n",
    "|--------|-------------|----------------------|------------------------|\n",
    "| ``Y``  | Year\t       | ± 9.2e18 years       | [9.2e18 BC, 9.2e18 AD] |\n",
    "| ``M``  | Month       | ± 7.6e17 years       | [7.6e17 BC, 7.6e17 AD] |\n",
    "| ``W``  | Week\t       | ± 1.7e17 years       | [1.7e17 BC, 1.7e17 AD] |\n",
    "| ``D``  | Day         | ± 2.5e16 years       | [2.5e16 BC, 2.5e16 AD] |\n",
    "| ``h``  | Hour        | ± 1.0e15 years       | [1.0e15 BC, 1.0e15 AD] |\n",
    "| ``m``  | Minute      | ± 1.7e13 years       | [1.7e13 BC, 1.7e13 AD] |\n",
    "| ``s``  | Second      | ± 2.9e12 years       | [ 2.9e9 BC, 2.9e9 AD]  |\n",
    "| ``ms`` | Millisecond | ± 2.9e9 years        | [ 2.9e6 BC, 2.9e6 AD]  |\n",
    "| ``us`` | Microsecond | ± 2.9e6 years        | [290301 BC, 294241 AD] |\n",
    "| ``ns`` | Nanosecond  | ± 292 years          | [ 1678 AD, 2262 AD]    |\n",
    "| ``ps`` | Picosecond  | ± 106 days           | [ 1969 AD, 1970 AD]    |\n",
    "| ``fs`` | Femtosecond | ± 2.6 hours          | [ 1969 AD, 1970 AD]    |\n",
    "| ``as`` | Attosecond  | ± 9.2 seconds        | [ 1969 AD, 1970 AD]    |\n",
    "\n",
    "### Dates and times in pandas: best of both worlds\n",
    "Pandas builds upon all the tools just discussed to provide a ``Timestamp`` object, which combines the ease-of-use of ``datetime`` and ``dateutil`` with the efficient storage and vectorized interface of ``numpy.datetime64``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flexibility of Python\n",
    "print(date := pd.to_datetime(\"4th of July, 2015\"))\n",
    "print(date.strftime('%A'))\n",
    "\n",
    "# power of NumPy vectorization\n",
    "print(date + pd.to_timedelta(np.arange(12), 'D'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Time Series: Indexing by Time\n",
    "\n",
    "Where the Pandas time series tools really become useful is when you begin to *index data by timestamps*.\n",
    "For example, we can construct a ``Series`` object that has time indexed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.DatetimeIndex(['2014-07-04', '2014-08-04',\n",
    "                          '2015-07-04', '2015-08-04'])\n",
    "data = pd.Series([0, 1, 2, 3], index=index)\n",
    "print(data)\n",
    "\n",
    "# slicing\n",
    "print(data['2014-07-04':'2015-07-04'])\n",
    "print(data['2015'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Time Series Data Structures\n",
    "\n",
    "This section will introduce the fundamental Pandas data structures for working with time series data:\n",
    "\n",
    "- For *time stamps*, Pandas provides the ``Timestamp`` type. As mentioned before, it is essentially a replacement for Python's native ``datetime``, but is based on the more efficient ``numpy.datetime64`` data type. The associated Index structure is ``DatetimeIndex``.\n",
    "- For *time Periods*, Pandas provides the ``Period`` type. This encodes a fixed-frequency interval based on ``numpy.datetime64``. The associated index structure is ``PeriodIndex``.\n",
    "- For *time deltas* or *durations*, Pandas provides the ``Timedelta`` type. ``Timedelta`` is a more efficient replacement for Python's native ``datetime.timedelta`` type, and is based on ``numpy.timedelta64``. The associated index structure is ``TimedeltaIndex``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DatetimeIndex creation: parsing available for a large variety of formats\n",
    "import datetime\n",
    "dates = pd.to_datetime([datetime.datetime(2015, 7, 3), '4th of July, 2015',\n",
    "                       '2015-Jul-6', '07-07-2015', '20150708'])\n",
    "print(dates)\n",
    "# or pd.date_range\n",
    "print(pd.date_range('2015-07-03', '2015-07-10'))\n",
    "\n",
    "# PeriodIndex from DatetimeIndex\n",
    "periods = dates.to_period('D')\n",
    "print(periods)\n",
    "# or pd.period_range\n",
    "print(pd.period_range('2015-07', periods=8, freq='M'))\n",
    "\n",
    "# TimedeltaIndex is created, for example, when a date is subtracted from another\n",
    "print(dates - dates[0])\n",
    "# or pd.timedelta_range\n",
    "print(pd.timedelta_range(0, periods=10, freq='H'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequencies and Offsets\n",
    "\n",
    "Fundamental to these Pandas time series tools is the concept of a frequency or date offset.\n",
    "Just as we saw the ``D`` (day) and ``H`` (hour) codes above, we can use such codes to specify any desired frequency spacing.\n",
    "The following table summarizes the main codes available:\n",
    "\n",
    "| Code   | Description         | Code   | Description          |\n",
    "|--------|---------------------|--------|----------------------|\n",
    "| ``D``  | Calendar day        | ``B``  | Business day         |\n",
    "| ``W``  | Weekly              |        |                      |\n",
    "| ``M``  | Month end           | ``BM`` | Business month end   |\n",
    "| ``Q``  | Quarter end         | ``BQ`` | Business quarter end |\n",
    "| ``A``  | Year end            | ``BA`` | Business year end    |\n",
    "| ``H``  | Hours               | ``BH`` | Business hours       |\n",
    "| ``T``  | Minutes             |        |                      |\n",
    "| ``S``  | Seconds             |        |                      |\n",
    "| ``L``  | Milliseonds         |        |                      |\n",
    "| ``U``  | Microseconds        |        |                      |\n",
    "| ``N``  | nanoseconds         |        |                      |\n",
    "\n",
    "\n",
    "### Frequency conversion\n",
    "The primary function for changing frequencies is the asfreq() method. For a DatetimeIndex, this is basically just a thin, but convenient wrapper around reindex() which generates a date_range and calls reindex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = pd.date_range(\"1/1/2010\", periods=3, freq=3 * pd.offsets.BDay())\n",
    "ts = pd.Series(np.random.randn(3), index=dr)\n",
    "print(ts)\n",
    "print(ts.asfreq(pd.offsets.BDay())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling\n",
    "\n",
    "pandas has a simple, powerful, and efficient functionality for performing resampling operations during frequency conversion (e.g., converting secondly data into 5-minutely data).\n",
    "\n",
    "resample() is a time-based groupby, followed by a reduction method on each of its groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling\n",
    "rng = pd.date_range(\"1/1/2012\", periods=300, freq=\"S\")\n",
    "ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)\n",
    "ts.resample(\"1Min\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsampling\n",
    "# from secondly to every 250 milliseconds\n",
    "print(ts[:2].resample(\"250L\").asfreq())\n",
    "print(ts[:2].resample(\"250L\").ffill())\n",
    "print(ts[:2].resample(\"250L\").ffill(limit=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "<span style='background: rgb(128, 128, 128, .15); width: 100%; display: block; padding: 10px 0 10px 10px'>< [Series & Dataframes](04.01-Dataframes.ipynb) | [Contents](00.00-Index.ipynb) | [Quiz 4](04.03-Quiz.ipynb) > [Top](#top) ^ </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background: rgb(128, 128, 128, .15); width: 100%; display: block; padding: 10px 0 10px 10px'>This is the Jupyter notebook version of the __Python for Official Statistics__ produced by Eurostat; the content is available [on GitHub](https://github.com/eurostat/e-learning/tree/main/python-official-statistics).\n",
    "<br>The text and code are released under the [EUPL-1.2 license](https://github.com/eurostat/e-learning/blob/main/LICENSE).</span>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "12d738f96725d1c1f433a1d40c5369c2dd6b861cec3a8aa29acd662c91ac2528"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
